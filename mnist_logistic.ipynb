{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Optical Recognition of Handwritten Digits Data Set\\n===================================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\nReferences\\n----------\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.DESCR\n",
    "#it is a subset data from mnist dataset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y=digits.target\n",
    "x=digits.data\n",
    "img=digits.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb4baa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb4baac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx, yy = np.meshgrid(np.arange(8), np.arange(8))\n",
    "plt.gray()\n",
    "plt.matshow(img[8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(y[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using sklearn's logistic regression\n",
    "import sklearn.linear_model\n",
    "clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finding accuracy ie correctly classifying any variable as its right class it belongs to.\n",
    "accuracy=0.0;\n",
    "for i in range(len(res)):\n",
    "    if res[i]==y[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= float(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860879243183083"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 178.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,  178.,    0.,    0.,    0.,    0.,    0.,    0.,    8.,\n",
       "           1.],\n",
       "       [   0.,    0.,  177.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,  180.,    0.,    0.,    0.,    0.,    1.,\n",
       "           1.],\n",
       "       [   0.,    0.,    0.,    0.,  181.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,  181.,    1.,    0.,    1.,\n",
       "           1.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,  179.,    0.,    1.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    1.,    0.,    0.,    0.,  178.,    0.,\n",
       "           0.],\n",
       "       [   0.,    3.,    0.,    2.,    0.,    0.,    1.,    0.,  163.,\n",
       "           0.],\n",
       "       [   0.,    1.,    0.,    0.,    0.,    1.,    0.,    1.,    0.,\n",
       "         177.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the confusion matrix shows us how each class is predicted correctly\n",
    "#the row headers are actual values and column headers are predicted classes\n",
    "confusion_mat=np.zeros([10,10])\n",
    "for i in range(len(res)):\n",
    "    confusion_mat[res[i]][y[i]] += 1.0\n",
    "confusion_mat\n",
    "#we see 1 is predicted as 8 many instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "##The sigmoid function adjusts the cost function hypothesis for binary classifiers exclusively.\n",
    "##for multiclass classifiers we make no.of models equal to the classes and the hypothesis\n",
    "##determines if it lies in that class or not\n",
    "def Sigmoid(z):\n",
    "    G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n",
    "    return G_of_Z \n",
    "#this softmax is used for ann here it has no use.\n",
    "def Softmax(z):\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This hypothesis will be used to calculate each instance of the Cost Function\n",
    "##it returns the predicted probablity of that data point classified as a class or not\n",
    "def Hypothesis(theta, x):\n",
    "    z = 0\n",
    "    for i in xrange(len(theta)):\n",
    "        z += (x[i]*theta[i])\n",
    "    prob=Sigmoid(z)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##For each member of the dataset, the result (Y) determines which variation of the cost function is used\n",
    "##The Y = 0 cost function punishes high probability estimations, and the Y = 1 it punishes low scores\n",
    "##The \"punishment\" makes the change in the gradient of ThetaCurrent - Average(CostFunction(Dataset)) greater\n",
    "##the cost function is sum(yi*log(hi)+(1-yi)*log(1-hi)) for all m data points average it and find the cost of each theta for \n",
    "##the error in the prediction\n",
    "def Cost_Function(X,Y,theta,m):\n",
    "    sumOfErrors = 0\n",
    "    for i in xrange(m):\n",
    "        xi = X[i]\n",
    "        hi = Hypothesis(theta,xi)\n",
    "        if Y[i] == 1:\n",
    "            error = Y[i] * math.log(hi)\n",
    "        elif Y[i] == 0:\n",
    "            error = (1-Y[i]) * math.log(1-hi)\n",
    "        sumOfErrors += error\n",
    "    const = -1/m\n",
    "    J = const * sumOfErrors\n",
    "    print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function find the error term for each theta value\n",
    "#error for a theta= sum((hi-yi)*xij)*alpha/m for m data points \n",
    "#j is the x[i] component multiplied with the respective theta term.\n",
    "def Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
    "    sumErrors = 0\n",
    "    for i in xrange(m):\n",
    "        xi = X[i]\n",
    "        xij = xi[j]\n",
    "        hi = Hypothesis(theta,X[i])\n",
    "        error = (hi - Y[i])*xij\n",
    "        sumErrors += error\n",
    "    m = len(Y)\n",
    "    constant = float(alpha)/float(m)\n",
    "    J = constant * sumErrors\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#thetaj=thetaj-d(cost function)/d(thetaj)\n",
    "#d(cost function)/d(thetaj) is the error found for each theta by Cost_Function_Derivative\n",
    "def Gradient_Descent(X,Y,theta,m,alpha):\n",
    "    new_theta = []\n",
    "    constant = alpha/m\n",
    "    for j in xrange(len(theta)):\n",
    "        CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
    "        new_theta_value = theta[j] - CFDerivative\n",
    "        new_theta.append(new_theta_value)\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in xrange(num_iters):\n",
    "        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "        theta = new_theta\n",
    "        #if x % 5 == 0:\n",
    "            #here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
    "            #Cost_Function(X,Y,theta,m)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict1 is the y term for each class dict1[class] shows the binary values signifying if the data is of that class or not\n",
    "initial_theta = [0.0 for i in range(64)]\n",
    "alpha = 0.1\n",
    "iterations = 3\n",
    "dict1={}\n",
    "for i in range(10):\n",
    "    dict1[i]=[]\n",
    "    for j in range(len(y)):\n",
    "        if y[j]== i:\n",
    "            dict1[i].append(1)\n",
    "        else:\n",
    "            dict1[i].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding weights for each pixel.....\n",
      "finished for digit no. 0.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 1.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 2.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 3.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 4.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 5.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 6.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 7.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 8.000000 \n",
      "finding weights for each pixel.....\n",
      "finished for digit no. 9.000000 \n"
     ]
    }
   ],
   "source": [
    "#finding theta for each classes classification.\n",
    "theta={}\n",
    "for i in range(10):\n",
    "    print('finding weights for each pixel.....')\n",
    "    theta[i]=Logistic_Regression(x,dict1[i],alpha,initial_theta,iterations)\n",
    "    print('finished for digit no. %f '%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finding the probablities by forward propagating in the hypothesis and finding the argument or jth term having largest probablity\n",
    "#this highest probablity argument is the class you can classify the data point into.\n",
    "probarg=[]\n",
    "for i in range(len(y)):\n",
    "    maxarg=0\n",
    "    maxval=0\n",
    "    for j in range(10):\n",
    "        well=Hypothesis(theta[j],x[i])\n",
    "        if well>maxval:\n",
    "            maxarg=j\n",
    "            maxval=well\n",
    "    probarg.append(maxarg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=0.0;\n",
    "for i in range(len(probarg)):\n",
    "    if probarg[i]==y[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= float(len(probarg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8836950473010573"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
